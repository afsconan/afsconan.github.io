<!DOCTYPE HTML>
<html>
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
    <meta name="Keywords" content="blog"/>
    <meta name="Description" content="blog"/>
    <title>Simple</title>
    <link rel="shortcut icon" href="/static/favicon.png"/>
    <link rel="stylesheet" type="text/css" href="/main.css" />
</head>
<body>
<div class="main">
    <div class="header">
    	<ul id="pages">
            <li><a href="/">home</a></li>
            <li><a href="/#/tags">tags</a></li>
            <li><a href="/#/archive">archive</a></li>
    	</ul>
    </div>
	<div class="wrap-header">
	<h1>
    <a href="/" id="title"></a>
	</h1>
	</div>
<div id="md" style="display: none;">
<!-- markdown -->
**一、Spark编程中的重要概念：**（具体实例  P10）

**1.弹性分布式数据集RDD（Resilient Distributed DataSets）**

**2.创建操作（creation operation）**  
RDD的*初始创建都是由SparkContext来负责*的，将内存中的集合或者外部文件系统作为输入源。  
**(1)集合创建操作**  
Spark中提供了parallelize和makeRDD两类函数来实现从集合生成RDD（两者不同之处，makeRDD还提供了一个可以指定每一个分区preferredLocations参数的实现版本）   
**(2)存储创建操作**  
Spark的整个生态系统与Hadoop是完全兼容的，所以对于Hadoop所支持的文件类型或者数据库类型，Spark也同样支持。  
由于Hadoop的API有新旧两个版本，Spark为了能够兼容Hadoop所有版本，也提供了两套创建操作接口。 
对于外部存储创建操作而言，hadoopRDD和newHadoopRDD是最为抽象的两个函数接口。
兼容旧版本Hadoop API的创建操作
textFile()    
hadoopFile…    
hadoopFile...      
sequenceFile…    
sequenceFile…    
objectFile…    
hadoopRDD…    

兼容新版本Hadoop API的创建操作
newAPIHadoopFile…  
newAPIHadoopFile…  
newAPIHadoopRDD…


**3.转换操作（transformation operation）**   
将一个RDD*通过一定的操作*变换成另一个RDD。          
**(1)RDD基本转换操作**   
map()   将RDD中类型为T的元素，一对一地映射为类型为U的元素  
distinct()  返回RDD中所有不一样的元素  
flatMap()  将RDD中的每一个元素进行*一对多*转换  
  
repartition()   是coalesce接口中shuffle为true的简易实现  
coalesce()    对RDD的分区进行重新划分  

randomSplit()  根据weights权重将一个RDD切分成多个RDD  
glom()  将RDD中每一个分区中类型为T的元素转换成数组Array[T]，这样每个分区就只有一个数组元素  

union()  针对RDD的集合操作  
intersection()  
intersection()  
subtract()  
subtract()  

mapPartitions()  与map转换操作类似，只不过映射函数的输入参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器  
mapPartitionsWithIndex()  

zip(…)  将两个RDD组合成Key/Value形式的RDD
zipPartitions(…)  将多个RDD按照partition组合成为新的RDD
zipPartitions(…)   
zipPartitions(…)  
zipPartitions(…)  
zipPartitions(…)  
zipPartitions(…)  

zipWithIndex()  将RDD中的元素和这个元素的ID组合成键/值对
zipWithUniqueId()  将RDD中的元素和一个唯一ID组合成键/值对

**(2)键值RDD转换操作**
partitionBy()  根据partitioner函数生成新的ShuffledRDD，将原RDD重新分区
mapValues()  针对[K,V]中的V值进行map操作
flatMapValues()  针对[K,V]中的V值进行flatMap操作

combineByKey(…) 将RDD[K,V]转化成返回类型RDD[K,C],这里V类型与C类型可以相同也可以不同  
combineByKey(…)  
combineByKey(…)  
foldByKey(…)  
foldByKey(…)  
foldByKey(…)  
reduceByKey(…)  
reduceByKey(…)  
reduceByKey(…)  
groupByKey(…)  
groupByKey(…)  
groupByKey(…)  

cogroup(…)  
cogroup(…)  
cogroup(…)  
join(…)  针对RDD[K,V]中K值相等的连接操作，内连接  
join(…)  
join(…)  
leftOuterJoin(…)  针对RDD[K,V]中K值相等的连接操作，左外连接  
leftOuterJoin(…)  
leftOuterJoin(…)  
rightOuterJoin(…)   针对RDD[K,V]中K值相等的连接操作，右外连接  
rightOuterJoin(…)  
rightOuterJoin(…)  
subtractByKey(…)  只是针对RDD[K,V]中K值来进行操作  
subtractByKey(…)  
subtractByKey(…)  

**4.控制操作  （control operation）**  
对RDD进行持久化，可以*让RDD保存在磁盘或者内存中*，以便后续重复使用。  

cache():RDD[T]    
persist():RDD[T]    
persist(level:StorageLevel):RDD[T]  

checkpoint()      &emsp;      checkpoint会割断此RDD之前的依赖关系，而persist接口依然保留着RDD的依赖关系


**5.行动操作（action operation）**  
由于Spark是惰性计算的，所以对于任何RDD进行行动操作，都会触发Spark作业的运行，从而产生最终的结果。   
Spark中的行动操作基本分为两类：一类的操作结果变成Scala集合或者标量，另一类就将RDD保存到外部文件或者数据库系统中。

**(1)集合标量行动操作**［行动操作将标量或者集合返回给Spark的客户端程序］  
first  
count  
reduce(f:(T,T)=>T)  对RDD中的元素进行二元计算，返回计算结果  

collect()/toArray()   以集合形式返回RDD的元素
take(num:Int)  
top(num:Int)    
takeOrdered(num:Int) &emsp; 以与top相反的排序规则返回

aggregate\[U](zeroValue:U)(seqOp:(U,T)=>U,combOp(U,U)=>U)

fold(zerovalue:T)(op:(T,T)=>T)  &emsp;   fold是aggregate的便利接口  

lookup(key:K):Seq[V]  &emsp;  针对(K,V)类型的RDD的行动操作，对于给定的键值，返回与此键值相对应的所有值。
  

**(2)存储行动操作** ［行动操作将RDD直接保存到外部文件系统或者数据库中］

将RDD存储到外部介质的旧版本API:
saveAsTextFile(...)  
saveAsTextFile(...)  
saveAsObjectFile(...)  
saveAsHadoopFile(...)  
saveAsHadoopFile(...)  
saveAsHadoopFile(...)  
outputFormatClass: Class...  
saveAsHadoopDataset(conf:JobConf)  

新版本Hadoop API:  
saveAsNewAPIHadoopFile(…)  
saveAsNewAPIHadoopFile(…)  
saveAsNewAPIHadoopDataset(conf:Configuration)  


**对于一个Spark数据处理程序而言，一般情况下，经过输入操作（创建操作）、转换操作、控制操作、输出操作（行动操作）来完成一个作业。**  
当然一个Spark应用程序中，可以有多个行动操作，也就是有多个作业存在。


**二、Spark RDD**  
一个RDD代表一个被分区的只读数据集。  
一个RDD的生成只有两种途径：  
（1）来自于内存集合和外部存储系统  
（2）通过转换操作来自于其他RDD（如map、filter、join等）  
**RDD是粗粒度的操作数据集**    （每一个转换操作都会生成一个新的RDD）（一个操作会被应用在RDD的所有数据上）    
**所以只要通过记录下这些作用在RDD之上的转化操作，来构建RDD的继承关系（ lineage），就可以有效地进行容错处理，而不需要将实际的RDD数据进行记录拷贝**  

**1.RDD分区（partitions）**  
可以指定多少分区，也可以不指定（采用系统默认的分区数----程序所分配到的资源的CPU核的个数）  
**2.RDD优先位置（preferredLocations）**   
preferredLocations返回每一个数据块所在的机器名或者 IP地址（如果每一块数据是多份存储的，那么就会返回多个机器地址）  
**3.RDD依赖关系（dependencies）** &emsp; PageRank算法的例子[多看]  
*窄依赖*：每一个父RDD的分区最多只被子RDD的一个分区所使用。  
*宽依赖*：多个子RDD的分区会依赖于同一个父RDD的分区。   
org.apache.spark.*OneToOneDependency*（窄依赖）
org.apache.spark.*ShuffleDependency* （宽依赖）  
**4.RDD分区计算（compute）**   
对于Spark中每个RDD的计算都是以partition(分区)为单位的，而且RDD中的compute函数都是在对迭代器进行复合，不需要保存每次计算的结果。    
**5.RDD分区函数（partitioner）**   &emsp; PageRank优化[多看]     
目前在Spark中实现了两种类型的分区函数：
HashPatitioner（哈希分区）和RangePatitioner（区域分区）

partitioner这个属性，只存在于（K,V）类型的RDD中，对于非（K,V）类型的partitioner的值就是None。










<!-- markdown end -->
</div>
<div class="entry" id="main">
<!-- content -->
<p><strong>一、Spark编程中的重要概念：</strong>（具体实例  P10）</p>

<p><strong>1.弹性分布式数据集RDD（Resilient Distributed DataSets）</strong></p>

<p><strong>2.创建操作（creation operation）</strong> <br>
RDD的<em>初始创建都是由SparkContext来负责</em>的，将内存中的集合或者外部文件系统作为输入源。 <br>
<strong>(1)集合创建操作</strong> <br>
Spark中提供了parallelize和makeRDD两类函数来实现从集合生成RDD（两者不同之处，makeRDD还提供了一个可以指定每一个分区preferredLocations参数的实现版本） <br>
<strong>(2)存储创建操作</strong> <br>
Spark的整个生态系统与Hadoop是完全兼容的，所以对于Hadoop所支持的文件类型或者数据库类型，Spark也同样支持。 <br>
由于Hadoop的API有新旧两个版本，Spark为了能够兼容Hadoop所有版本，也提供了两套创建操作接口。 
对于外部存储创建操作而言，hadoopRDD和newHadoopRDD是最为抽象的两个函数接口。
兼容旧版本Hadoop API的创建操作
textFile() <br>
hadoopFile… <br>
hadoopFile... <br>
sequenceFile… <br>
sequenceFile… <br>
objectFile… <br>
hadoopRDD…    </p>

<p>兼容新版本Hadoop API的创建操作
newAPIHadoopFile… <br>
newAPIHadoopFile… <br>
newAPIHadoopRDD…</p>

<p><strong>3.转换操作（transformation operation）</strong> <br>
将一个RDD<em>通过一定的操作</em>变换成另一个RDD。 <br>
<strong>(1)RDD基本转换操作</strong> <br>
map()   将RDD中类型为T的元素，一对一地映射为类型为U的元素 <br>
distinct()  返回RDD中所有不一样的元素 <br>
flatMap()  将RDD中的每一个元素进行<em>一对多</em>转换  </p>

<p>repartition()   是coalesce接口中shuffle为true的简易实现 <br>
coalesce()    对RDD的分区进行重新划分  </p>

<p>randomSplit()  根据weights权重将一个RDD切分成多个RDD <br>
glom()  将RDD中每一个分区中类型为T的元素转换成数组Array[T]，这样每个分区就只有一个数组元素  </p>

<p>union()  针对RDD的集合操作 <br>
intersection() <br>
intersection() <br>
subtract() <br>
subtract()  </p>

<p>mapPartitions()  与map转换操作类似，只不过映射函数的输入参数由RDD中的每一个元素变成了RDD中每一个分区的迭代器 <br>
mapPartitionsWithIndex()  </p>

<p>zip(…)  将两个RDD组合成Key/Value形式的RDD
zipPartitions(…)  将多个RDD按照partition组合成为新的RDD
zipPartitions(…) <br>
zipPartitions(…) <br>
zipPartitions(…) <br>
zipPartitions(…) <br>
zipPartitions(…)  </p>

<p>zipWithIndex()  将RDD中的元素和这个元素的ID组合成键/值对
zipWithUniqueId()  将RDD中的元素和一个唯一ID组合成键/值对</p>

<p><strong>(2)键值RDD转换操作</strong>
partitionBy()  根据partitioner函数生成新的ShuffledRDD，将原RDD重新分区
mapValues()  针对[K,V]中的V值进行map操作
flatMapValues()  针对[K,V]中的V值进行flatMap操作</p>

<p>combineByKey(…) 将RDD[K,V]转化成返回类型RDD[K,C],这里V类型与C类型可以相同也可以不同 <br>
combineByKey(…) <br>
combineByKey(…) <br>
foldByKey(…) <br>
foldByKey(…) <br>
foldByKey(…) <br>
reduceByKey(…) <br>
reduceByKey(…) <br>
reduceByKey(…) <br>
groupByKey(…) <br>
groupByKey(…) <br>
groupByKey(…)  </p>

<p>cogroup(…) <br>
cogroup(…) <br>
cogroup(…) <br>
join(…)  针对RDD[K,V]中K值相等的连接操作，内连接 <br>
join(…) <br>
join(…) <br>
leftOuterJoin(…)  针对RDD[K,V]中K值相等的连接操作，左外连接 <br>
leftOuterJoin(…) <br>
leftOuterJoin(…) <br>
rightOuterJoin(…)   针对RDD[K,V]中K值相等的连接操作，右外连接 <br>
rightOuterJoin(…) <br>
rightOuterJoin(…) <br>
subtractByKey(…)  只是针对RDD[K,V]中K值来进行操作 <br>
subtractByKey(…) <br>
subtractByKey(…)  </p>

<p><strong>4.控制操作  （control operation）</strong> <br>
对RDD进行持久化，可以<em>让RDD保存在磁盘或者内存中</em>，以便后续重复使用。  </p>

<p>cache():RDD[T] <br>
persist():RDD[T] <br>
persist(level:StorageLevel):RDD[T]  </p>

<p>checkpoint()             checkpoint会割断此RDD之前的依赖关系，而persist接口依然保留着RDD的依赖关系</p>

<p><strong>5.行动操作（action operation）</strong> <br>
由于Spark是惰性计算的，所以对于任何RDD进行行动操作，都会触发Spark作业的运行，从而产生最终的结果。 <br>
Spark中的行动操作基本分为两类：一类的操作结果变成Scala集合或者标量，另一类就将RDD保存到外部文件或者数据库系统中。</p>

<p><strong>(1)集合标量行动操作</strong>［行动操作将标量或者集合返回给Spark的客户端程序］ <br>
first <br>
count <br>
reduce(f:(T,T)=&gt;T)  对RDD中的元素进行二元计算，返回计算结果  </p>

<p>collect()/toArray()   以集合形式返回RDD的元素
take(num:Int) <br>
top(num:Int) <br>
takeOrdered(num:Int)   以与top相反的排序规则返回</p>

<p>aggregate[U](zeroValue:U)(seqOp:(U,T)=&gt;U,combOp(U,U)=&gt;U)</p>

<p>fold(zerovalue:T)(op:(T,T)=&gt;T)      fold是aggregate的便利接口  </p>

<p>lookup(key:K):Seq[V]     针对(K,V)类型的RDD的行动操作，对于给定的键值，返回与此键值相对应的所有值。</p>

<p><strong>(2)存储行动操作</strong> ［行动操作将RDD直接保存到外部文件系统或者数据库中］</p>

<p>将RDD存储到外部介质的旧版本API:
saveAsTextFile(...) <br>
saveAsTextFile(...) <br>
saveAsObjectFile(...) <br>
saveAsHadoopFile(...) <br>
saveAsHadoopFile(...) <br>
saveAsHadoopFile(...) <br>
outputFormatClass: Class... <br>
saveAsHadoopDataset(conf:JobConf)  </p>

<p>新版本Hadoop API: <br>
saveAsNewAPIHadoopFile(…) <br>
saveAsNewAPIHadoopFile(…) <br>
saveAsNewAPIHadoopDataset(conf:Configuration)  </p>

<p><strong>对于一个Spark数据处理程序而言，一般情况下，经过输入操作（创建操作）、转换操作、控制操作、输出操作（行动操作）来完成一个作业。</strong> <br>
当然一个Spark应用程序中，可以有多个行动操作，也就是有多个作业存在。</p>

<p><strong>二、Spark RDD</strong> <br>
一个RDD代表一个被分区的只读数据集。 <br>
一个RDD的生成只有两种途径： <br>
（1）来自于内存集合和外部存储系统 <br>
（2）通过转换操作来自于其他RDD（如map、filter、join等） <br>
<strong>RDD是粗粒度的操作数据集</strong>    （每一个转换操作都会生成一个新的RDD）（一个操作会被应用在RDD的所有数据上） <br>
<strong>所以只要通过记录下这些作用在RDD之上的转化操作，来构建RDD的继承关系（ lineage），就可以有效地进行容错处理，而不需要将实际的RDD数据进行记录拷贝</strong>  </p>

<p><strong>1.RDD分区（partitions）</strong> <br>
可以指定多少分区，也可以不指定（采用系统默认的分区数----程序所分配到的资源的CPU核的个数） <br>
<strong>2.RDD优先位置（preferredLocations）</strong> <br>
preferredLocations返回每一个数据块所在的机器名或者 IP地址（如果每一块数据是多份存储的，那么就会返回多个机器地址） <br>
<strong>3.RDD依赖关系（dependencies）</strong>   PageRank算法的例子[多看] <br>
<em>窄依赖</em>：每一个父RDD的分区最多只被子RDD的一个分区所使用。 <br>
<em>宽依赖</em>：多个子RDD的分区会依赖于同一个父RDD的分区。 <br>
org.apache.spark.<em>OneToOneDependency</em>（窄依赖）
org.apache.spark.<em>ShuffleDependency</em> （宽依赖） <br>
<strong>4.RDD分区计算（compute）</strong> <br>
对于Spark中每个RDD的计算都是以partition(分区)为单位的，而且RDD中的compute函数都是在对迭代器进行复合，不需要保存每次计算的结果。 <br>
<strong>5.RDD分区函数（partitioner）</strong>     PageRank优化[多看] <br>
目前在Spark中实现了两种类型的分区函数：
HashPatitioner（哈希分区）和RangePatitioner（区域分区）</p>

<p>partitioner这个属性，只存在于（K,V）类型的RDD中，对于非（K,V）类型的partitioner的值就是None。</p>
<!-- content end -->
</div>
<br>
<br>
    <div id="disqus_thread"></div>
	<div class="footer">
		<p>© Copyright 2014 by isnowfy, Designed by isnowfy</p>
	</div>
</div>
<script src="main.js"></script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ["\\(", "\\)"]], processEscapes: true}});
</script>
<script id="content" type="text/mustache">
    <h1>{{title}}</h1>
    <div class="tag">
    {{date}}
    {{#tags}}
    <a href="/#/tag/{{name}}">#{{name}}</a>
    {{/tags}}
    </div>
</script>
<script id="pagesTemplate" type="text/mustache">
    {{#pages}}
    <li>
        <a href="{{path}}">{{title}}</a>
    </li>
    {{/pages}}
</script>
<script>
$(document).ready(function() {
    $.ajax({
        url: "main.json",
        type: "GET",
        dataType: "json",
        success: function(data) {
            $("#title").html(data.name);
            var pagesTemplate = Hogan.compile($("#pagesTemplate").html());
            var pagesHtml = pagesTemplate.render({"pages": data.pages});
            $("#pages").append(pagesHtml);
            //path
            var path = "rdd_interface.html";
            //path end
            var now = 0;
            for (var i = 0; i < data.posts.length; ++i)
                if (path == data.posts[i].path)
                    now = i;
            var post = data.posts[now];
            var tmp = post.tags.split(" ");
            var tags = [];
            for (var i = 0; i < tmp.length; ++i)
                if (tmp[i].length > 0)
                    tags.push({"name": tmp[i]});
            var contentTemplate = Hogan.compile($("#content").html());
            var contentHtml = contentTemplate.render({"title": post.title, "tags": tags, "date": post.date});
            $("#main").prepend(contentHtml);
            if (data.disqus_shortname.length > 0) {
                var disqus_shortname = data.disqus_shortname;
                (function() {
                    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
                    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
                    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
                })();
            }
        }
    });
});
</script>
</body>
</html>
